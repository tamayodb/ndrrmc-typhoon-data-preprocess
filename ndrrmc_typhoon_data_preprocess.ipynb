{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXyEOfuX3+3LdW3Hf6tYo9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tamayodb/ndrrmc-typhoon-data-preprocess/blob/main/ndrrmc_typhoon_data_preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "efgR3HmufuWM",
        "outputId": "5714ea9e-e640-4041-8e70-830f5e572678"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3e493e84-a87f-4953-ae8e-1ee920464a51\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3e493e84-a87f-4953-ae8e-1ee920464a51\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 2020-QUINTA.xlsx to 2020-QUINTA (1).xlsx\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from difflib import SequenceMatcher\n",
        "from datetime import datetime\n",
        "\n",
        "def normalize_location_name(name, keep_parentheses=False):\n",
        "    \"\"\"Comprehensive location name normalization with parentheses handling\"\"\"\n",
        "    if pd.isna(name):\n",
        "        return name\n",
        "\n",
        "    name = str(name).strip()\n",
        "\n",
        "    # Remove common prefixes/suffixes\n",
        "    name = re.sub(r'^(city of|municipality of|province of)\\s+', '', name, flags=re.IGNORECASE)\n",
        "\n",
        "    # Handle parentheses - extract base name unless keep_parentheses=True\n",
        "    if not keep_parentheses:\n",
        "\n",
        "        base_name = re.sub(r'\\s*\\([^)]*\\).*', '', name).strip()\n",
        "        if base_name:\n",
        "            name = base_name\n",
        "\n",
        "    # Standardize separators\n",
        "    name = name.replace('-', ' ')\n",
        "    name = name.replace('_', ' ')\n",
        "    name = name.replace('.', '')\n",
        "\n",
        "    # Handle common abbreviations\n",
        "    abbreviations = {\n",
        "        ' st ': ' saint ',\n",
        "        ' st.': ' saint',\n",
        "        ' sto ': ' santo ',\n",
        "        ' sto.': ' santo',\n",
        "        ' sta ': ' santa ',\n",
        "        ' sta.': ' santa',\n",
        "        ' n ': ' north ',\n",
        "        ' s ': ' south ',\n",
        "        ' e ': ' east ',\n",
        "        ' w ': ' west ',\n",
        "    }\n",
        "\n",
        "    name_lower = name.lower()\n",
        "    for abbr, full in abbreviations.items():\n",
        "        name_lower = name_lower.replace(abbr, full)\n",
        "\n",
        "    # Remove extra whitespace and standardize case\n",
        "    name = ' '.join(name_lower.split())\n",
        "    name = name.title()  # Proper case\n",
        "\n",
        "    return name\n",
        "\n",
        "def create_location_mapping(df):\n",
        "    \"\"\"Create a mapping for similar location names to handle parentheses cases\"\"\"\n",
        "    location_col = 'City/Municipality'\n",
        "    if location_col not in df.columns:\n",
        "        return {}\n",
        "\n",
        "    locations = df[location_col].dropna().unique()\n",
        "    mapping = {}\n",
        "\n",
        "    for loc in locations:\n",
        "        base_name = normalize_location_name(loc, keep_parentheses=False)\n",
        "        if base_name != loc:\n",
        "            mapping[loc] = base_name\n",
        "            print(f\" Mapping: '{loc}' → '{base_name}'\")\n",
        "\n",
        "    return mapping\n",
        "\n",
        "def clean_typhoon_name(name):\n",
        "    \"\"\"Clean and standardize typhoon names\"\"\"\n",
        "    if pd.isna(name):\n",
        "        return name\n",
        "\n",
        "    name = str(name).strip().upper()\n",
        "\n",
        "    # Remove common prefixes\n",
        "    name = re.sub(r'^(TYPHOON|TY|TROPICAL STORM|TS)\\s+', '', name)\n",
        "\n",
        "    # Handle parentheses and additional info\n",
        "    name = re.sub(r'\\s*\\([^)]*\\)', '', name)\n",
        "    name = re.sub(r'\\s*\\d{4}.*', '', name)\n",
        "\n",
        "    return name.strip()\n",
        "\n",
        "def standardize_numeric_columns(df, numeric_cols):\n",
        "    \"\"\"Clean and standardize numeric columns\"\"\"\n",
        "    for col in numeric_cols:\n",
        "        if col not in df.columns:\n",
        "            continue\n",
        "\n",
        "        # Convert to string first to handle mixed types\n",
        "        df[col] = df[col].astype(str)\n",
        "\n",
        "        # Remove common non-numeric characters\n",
        "        df[col] = df[col].str.replace(',', '')  # Remove commas\n",
        "        df[col] = df[col].str.replace('₱', '')  # Remove peso sign\n",
        "        df[col] = df[col].str.replace('PHP', '', case=False)\n",
        "        df[col] = df[col].str.replace('$', '')\n",
        "        df[col] = df[col].str.replace(' ', '')  # Remove spaces\n",
        "\n",
        "        # Handle common text values\n",
        "        df[col] = df[col].str.replace('none', '0', case=False)\n",
        "        df[col] = df[col].str.replace('nil', '0', case=False)\n",
        "        df[col] = df[col].str.replace('n/a', '0', case=False)\n",
        "        df[col] = df[col].str.replace('na', '0', case=False)\n",
        "        df[col] = df[col].str.replace('-', '0')\n",
        "\n",
        "        # Convert to numeric, replacing non-convertible with 0\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
        "\n",
        "        # Handle negative values (set to 0 as they're likely data entry errors)\n",
        "        df[col] = df[col].clip(lower=0)\n",
        "\n",
        "    return df\n",
        "\n",
        "def clean_assistance_type(assistance_type):\n",
        "    \"\"\"Standardize assistance type names\"\"\"\n",
        "    if pd.isna(assistance_type):\n",
        "        return assistance_type\n",
        "\n",
        "    assistance_type = str(assistance_type).strip()\n",
        "\n",
        "    # Standardize common assistance types\n",
        "    type_mapping = {\n",
        "        'ffp': 'Family Food Pack',\n",
        "        'family food packs': 'Family Food Pack',\n",
        "        'food pack': 'Family Food Pack',\n",
        "        'relief goods': 'Relief Goods',\n",
        "        'relief good': 'Relief Goods',\n",
        "        'financial assistance': 'Financial',\n",
        "        'cash assistance': 'Financial',\n",
        "        'financial aid': 'Financial',\n",
        "        'medicine': 'Medical Assistance',\n",
        "        'medical supplies': 'Medical Assistance',\n",
        "        'hygiene kit': 'Hygiene Kit',\n",
        "        'hygiene kits': 'Hygiene Kit',\n",
        "        'sleeping kit': 'Sleeping Kit',\n",
        "        'sleeping kits': 'Sleeping Kit',\n",
        "    }\n",
        "\n",
        "    assistance_lower = assistance_type.lower()\n",
        "    for key, value in type_mapping.items():\n",
        "        if key in assistance_lower:\n",
        "            return value\n",
        "\n",
        "    # If no mapping found, return title case\n",
        "    return assistance_type.title()\n",
        "\n",
        "def validate_and_clean_year(year):\n",
        "    \"\"\"Validate and clean year values\"\"\"\n",
        "    if pd.isna(year):\n",
        "        return year\n",
        "\n",
        "    try:\n",
        "        year = int(float(str(year)))\n",
        "        # Reasonable range for typhoon data\n",
        "        if 2020 <= year <= datetime.now().year:\n",
        "            return year\n",
        "        else:\n",
        "            return np.nan\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "def preprocess_dataframe(df, sheet_name):\n",
        "    \"\"\"Apply comprehensive preprocessing to a dataframe\"\"\"\n",
        "    print(f\"  Preprocessing {sheet_name}...\")\n",
        "\n",
        "    # Store original shape\n",
        "    original_shape = df.shape\n",
        "\n",
        "    # 1. Clean column names\n",
        "    df.columns = df.columns.str.strip()\n",
        "\n",
        "    # 2. Remove completely empty rows and columns\n",
        "    df = df.dropna(how='all')  # Remove empty rows\n",
        "    df = df.loc[:, df.notna().any()]  # Remove empty columns\n",
        "\n",
        "    # 3. Create location mapping for parentheses cases\n",
        "    if 'City/Municipality' in df.columns:\n",
        "        location_mapping = create_location_mapping(df)\n",
        "        # Apply the mapping\n",
        "        df['City/Municipality'] = df['City/Municipality'].replace(location_mapping)\n",
        "\n",
        "    # 4. Standardize location names\n",
        "    location_cols = ['Province', 'City/Municipality', 'Region']\n",
        "    for col in location_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].apply(lambda x: normalize_location_name(x, keep_parentheses=False))\n",
        "\n",
        "    # 5. Clean typhoon names\n",
        "    if 'Typhoon Name' in df.columns:\n",
        "        df['Typhoon Name'] = df['Typhoon Name'].apply(clean_typhoon_name)\n",
        "\n",
        "    # 6. Validate and clean years\n",
        "    if 'Year' in df.columns:\n",
        "        df['Year'] = df['Year'].apply(validate_and_clean_year)\n",
        "\n",
        "    # 7. Handle numeric columns based on sheet type\n",
        "    numeric_cols = []\n",
        "    if sheet_name == \"Affected Population\":\n",
        "        numeric_cols = ['Families', 'Person', 'Brgy']\n",
        "    elif sheet_name == \"Casualties\":\n",
        "        numeric_cols = ['Dead', 'Injured/Ill', 'Missing']\n",
        "    elif sheet_name == \"Damaged Houses\":\n",
        "        numeric_cols = ['Totally', 'Partially', 'Total']\n",
        "    elif sheet_name == \"Assistance Provided\":\n",
        "        numeric_cols = ['Quantity', 'Cost']\n",
        "        # Special handling for assistance type\n",
        "        if 'Type' in df.columns:\n",
        "            df['Type'] = df['Type'].apply(clean_assistance_type)\n",
        "\n",
        "    df = standardize_numeric_columns(df, numeric_cols)\n",
        "\n",
        "    # 8. Remove duplicate records (but aggregate them properly)\n",
        "    key_cols = ['Typhoon Name', 'Year', 'Region', 'Province', 'City/Municipality']\n",
        "    available_keys = [col for col in key_cols if col in df.columns]\n",
        "\n",
        "    if sheet_name != \"Assistance Provided\":  # Don't remove duplicates from assistance data\n",
        "        # Instead of just dropping duplicates, aggregate them\n",
        "        if available_keys:\n",
        "            numeric_cols_in_df = [col for col in numeric_cols if col in df.columns]\n",
        "            text_cols_in_df = [col for col in df.columns if col not in available_keys + numeric_cols_in_df]\n",
        "\n",
        "            agg_funcs = {}\n",
        "            for col in numeric_cols_in_df:\n",
        "                agg_funcs[col] = 'sum'\n",
        "            for col in text_cols_in_df:\n",
        "                agg_funcs[col] = lambda x: ', '.join(x.dropna().astype(str).unique()) if len(x.dropna()) > 0 else np.nan\n",
        "\n",
        "            if agg_funcs:\n",
        "                before_agg = len(df)\n",
        "                df = df.groupby(available_keys, as_index=False).agg(agg_funcs)\n",
        "                after_agg = len(df)\n",
        "                if before_agg != after_agg:\n",
        "                    print(f\" Aggregated {before_agg} records into {after_agg} records\")\n",
        "\n",
        "    # 9. Data validation\n",
        "    validation_issues = []\n",
        "\n",
        "    # Check for missing key information\n",
        "    for col in available_keys:\n",
        "        missing_count = df[col].isna().sum()\n",
        "        if missing_count > 0:\n",
        "            validation_issues.append(f\"{col}: {missing_count} missing values\")\n",
        "\n",
        "    # Check for outliers in numeric columns\n",
        "    for col in numeric_cols:\n",
        "        if col in df.columns:\n",
        "            q99 = df[col].quantile(0.99)\n",
        "            if q99 > 0:  # Avoid division by zero\n",
        "                outliers = (df[col] > q99 * 10).sum()  # Values 10x larger than 99th percentile\n",
        "                if outliers > 0:\n",
        "                    validation_issues.append(f\"{col}: {outliers} potential outliers\")\n",
        "\n",
        "    if validation_issues:\n",
        "        print(f\"    Data quality issues: {'; '.join(validation_issues)}\")\n",
        "\n",
        "    print(f\"   Shape: {original_shape} → {df.shape}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Main processing\n",
        "file_path = \"/content/2020-QUINTA.xlsx\"\n",
        "xls = pd.ExcelFile(file_path)\n",
        "\n",
        "print(\"Starting comprehensive data preprocessing...\")\n",
        "\n",
        "# --- Define sheets we want ---\n",
        "sheets = {\n",
        "    \"Affected Population\": [\"Families\", \"Person\", \"Brgy\"],\n",
        "    \"Casualties\": [\"Dead\", \"Injured/Ill\", \"Missing\"],\n",
        "    \"Damaged Houses\": [\"Totally\", \"Partially\", \"Total\"],\n",
        "}\n",
        "\n",
        "# --- Key columns for merging ---\n",
        "key_cols = [\"Typhoon Name\", \"Year\", \"Region\", \"Province\", \"City/Municipality\"]\n",
        "\n",
        "merged = None\n",
        "\n",
        "# Process sheets that can be aggregated\n",
        "for sheet, cols in sheets.items():\n",
        "    print(f\"\\n Processing {sheet}...\")\n",
        "    df = pd.read_excel(file_path, sheet_name=sheet)\n",
        "\n",
        "    # Apply preprocessing\n",
        "    df = preprocess_dataframe(df, sheet)\n",
        "\n",
        "    # Keep only relevant columns\n",
        "    keep_cols = [c for c in key_cols + cols if c in df.columns]\n",
        "    df = df[keep_cols]\n",
        "\n",
        "    # Group by keys for final aggregation\n",
        "    numeric_cols = [c for c in cols if c in df.columns and df[c].dtype in ['int64', 'float64']]\n",
        "    text_cols = [c for c in cols if c in df.columns and df[c].dtype == 'object']\n",
        "\n",
        "    agg_funcs = {}\n",
        "    for col in numeric_cols:\n",
        "        agg_funcs[col] = 'sum'\n",
        "    for col in text_cols:\n",
        "        agg_funcs[col] = lambda x: ', '.join(x.dropna().astype(str).unique())\n",
        "\n",
        "    if agg_funcs:\n",
        "        df = df.groupby([col for col in key_cols if col in df.columns], as_index=False).agg(agg_funcs)\n",
        "    else:\n",
        "        df = df.drop_duplicates(subset=[col for col in key_cols if col in df.columns])\n",
        "\n",
        "    # Merge with main table\n",
        "    if merged is None:\n",
        "        merged = df\n",
        "    else:\n",
        "        merge_keys = [col for col in key_cols if col in merged.columns and col in df.columns]\n",
        "        merged = pd.merge(merged, df, on=merge_keys, how=\"outer\")\n",
        "\n",
        "# Process assistance data\n",
        "print(f\"\\n Processing Assistance Provided...\")\n",
        "assistance_df = pd.read_excel(file_path, sheet_name=\"Assistance Provided\")\n",
        "assistance_data = preprocess_dataframe(assistance_df, \"Assistance Provided\")\n",
        "\n",
        "# Keep relevant assistance columns\n",
        "assistance_cols = [\"Quantity\", \"Type\", \"Cost\"]\n",
        "assistance_keep_cols = [c for c in key_cols + assistance_cols if c in assistance_data.columns]\n",
        "assistance_data = assistance_data[assistance_keep_cols]\n",
        "\n",
        "# Final merge\n",
        "print(f\"\\n Performing final merge...\")\n",
        "merge_keys = [col for col in key_cols if col in merged.columns and col in assistance_data.columns]\n",
        "final_merged = pd.merge(merged, assistance_data, on=merge_keys, how=\"outer\")\n",
        "\n",
        "# Final data quality report\n",
        "print(f\"\\n Final Data Quality Report:\")\n",
        "print(f\"  • Total records: {len(final_merged):,}\")\n",
        "print(f\"  • Unique typhoons: {final_merged['Typhoon Name'].nunique()}\")\n",
        "print(f\"  • Year range: {final_merged['Year'].min():.0f} - {final_merged['Year'].max():.0f}\")\n",
        "print(f\"  • Provinces covered: {final_merged['Province'].nunique()}\")\n",
        "print(f\"  • Cities/Municipalities: {final_merged['City/Municipality'].nunique()}\")\n",
        "\n",
        "if 'Type' in final_merged.columns:\n",
        "    print(f\"  • Assistance types: {final_merged['Type'].nunique()}\")\n",
        "    print(f\"  • Top assistance types: {final_merged['Type'].value_counts().head(3).to_dict()}\")\n",
        "\n",
        "# Save results | Naming convention - merged_typhoon_year_data_cleaned\n",
        "final_merged.to_excel(\"/content/merged_typhoon_2020_data_cleaned.xlsx\", index=False)\n",
        "print(\"Cleaned and merged file saved: merged_typhoon_2020_data_cleaned.xlsx\")\n",
        "\n",
        "# Show sample for verification\n",
        "print(f\"\\n Sample of cleaned data:\")\n",
        "sample_cols = ['Typhoon Name', 'Year', 'Province', 'City/Municipality', 'Type', 'Quantity', 'Cost']\n",
        "display_cols = [col for col in sample_cols if col in final_merged.columns]\n",
        "print(final_merged[display_cols].head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTyIkNA5f1Wq",
        "outputId": "b1ccd492-9aed-4759-bb47-2158531a7713"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting comprehensive data preprocessing...\n",
            "\n",
            " Processing Affected Population...\n",
            "  Preprocessing Affected Population...\n",
            " Mapping: 'Dingalan ' → 'Dingalan'\n",
            " Mapping: 'Mendez (Mendez-Nuñez)' → 'Mendez'\n",
            " Mapping: 'Tobias Fornier (Dao)' → 'Tobias Fornier'\n",
            " Mapping: 'Sta. Cruz' → 'Sta Cruz'\n",
            " Mapping: 'Sta. Fe' → 'Sta Fe'\n",
            " Mapping: 'Sta. Maria' → 'Sta Maria'\n",
            " Mapping: 'Rapu-Rapu' → 'Rapu Rapu'\n",
            " Mapping: 'Pio V. Corpuz' → 'Pio V Corpuz'\n",
            " Mapping: 'Sta. Magdalena' → 'Sta Magdalena'\n",
            " Mapping: 'Laua-an' → 'Laua An'\n",
            " Mapping: 'San Jose de Buenavista' → 'San Jose De Buenavista'\n",
            " Mapping: 'Hinoba-an (Asia)' → 'Hinoba An'\n",
            " Mapping: 'Moises Padilla (Magallon)' → 'Moises Padilla'\n",
            " Mapping: 'Bayawan (Tulong)' → 'Bayawan'\n",
            "   Shape: (337, 8) → (337, 8)\n",
            "\n",
            " Processing Casualties...\n",
            "  Preprocessing Casualties...\n",
            " Mapping: 'Sanchez-Mira' → 'Sanchez Mira'\n",
            " Mapping: 'San Andres (Calolbon)' → 'San Andres'\n",
            " Mapping: 'Tobias Fornier (Dao)' → 'Tobias Fornier'\n",
            " Aggregated 56 records into 35 records\n",
            "   Shape: (141, 8) → (35, 8)\n",
            "\n",
            " Processing Damaged Houses...\n",
            "  Preprocessing Damaged Houses...\n",
            " Mapping: 'San Francisco (Aurora)' → 'San Francisco'\n",
            " Mapping: 'Abra de Ilog' → 'Abra De Ilog'\n",
            " Mapping: 'Sta. Cruz' → 'Sta Cruz'\n",
            " Mapping: 'Rapu-Rapu' → 'Rapu Rapu'\n",
            " Mapping: 'Tobias Fornier (Dao)' → 'Tobias Fornier'\n",
            " Mapping: 'Hinoba-an (Asia)' → 'Hinoba An'\n",
            " Mapping: 'Bayawan (Tulong)' → 'Bayawan'\n",
            "   Shape: (141, 8) → (141, 8)\n",
            "\n",
            " Processing Assistance Provided...\n",
            "  Preprocessing Assistance Provided...\n",
            " Mapping: 'Abra de Ilog' → 'Abra De Ilog'\n",
            " Mapping: 'Bulalacao (San Pedro)' → 'Bulalacao'\n",
            " Mapping: 'Sta. Maria' → 'Sta Maria'\n",
            " Mapping: 'Laua-an' → 'Laua An'\n",
            " Mapping: 'San Jose de Buenavista' → 'San Jose De Buenavista'\n",
            " Mapping: 'Hinoba-an (Asia)' → 'Hinoba An'\n",
            "    Data quality issues: City/Municipality: 7 missing values\n",
            "   Shape: (92, 8) → (92, 8)\n",
            "\n",
            " Performing final merge...\n",
            "\n",
            " Final Data Quality Report:\n",
            "  • Total records: 379\n",
            "  • Unique typhoons: 1\n",
            "  • Year range: 2020 - 2020\n",
            "  • Provinces covered: 34\n",
            "  • Cities/Municipalities: 316\n",
            "  • Assistance types: 13\n",
            "  • Top assistance types: {'Financial': 30, 'Family Food Pack': 23, 'Not Specified': 16}\n",
            "Cleaned and merged file saved: merged_typhoon_2020_data_cleaned.xlsx\n",
            "\n",
            " Sample of cleaned data:\n",
            "  Typhoon Name    Year      Province City/Municipality              Type  \\\n",
            "0       QUINTA  2020.0  Ilocos Norte          Pagudpud         Financial   \n",
            "1       QUINTA  2020.0       Cagayan            Abulug  Family Food Pack   \n",
            "2       QUINTA  2020.0       Cagayan         Allacapan               NaN   \n",
            "3       QUINTA  2020.0       Cagayan            Baggao               NaN   \n",
            "4       QUINTA  2020.0       Cagayan          Claveria  Family Food Pack   \n",
            "\n",
            "   Quantity      Cost  \n",
            "0       0.0  267000.0  \n",
            "1       8.0    3763.6  \n",
            "2       NaN       NaN  \n",
            "3       NaN       NaN  \n",
            "4    1500.0  705225.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1GCVbarpgeK8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}